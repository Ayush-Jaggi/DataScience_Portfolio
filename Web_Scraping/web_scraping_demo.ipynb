{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Demo\n",
    "\n",
    "This notebook demonstrates how to scrape quotes from quotes.toscrape.com using Python, requests, and BeautifulSoup.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand HTTP requests and responses\n",
    "- Parse HTML content with BeautifulSoup\n",
    "- Extract structured data from web pages\n",
    "- Handle pagination and multiple pages\n",
    "- Save scraped data to CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom web scraper\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from web_scraper import WebScraper\n",
    "\n",
    "print(\"Web scraper imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize the Web Scraper\n",
    "\n",
    "Let's create an instance of our WebScraper class with appropriate settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scraper\n",
    "scraper = WebScraper(\"http://quotes.toscrape.com\", delay=1.0)\n",
    "print(\"Scraper initialized with 1-second delay between requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test Single Page Scraping\n",
    "\n",
    "Let's first test scraping a single page to understand the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first page\n",
    "soup = scraper.get_page(\"http://quotes.toscrape.com/page/1/\")\n",
    "\n",
    "if soup:\n",
    "    quotes = soup.find_all('div', class_='quote')\n",
    "    print(f\"Found {len(quotes)} quotes on the first page\")\n",
    "    \n",
    "    # Display first quote as example\n",
    "    if quotes:\n",
    "        first_quote = quotes[0]\n",
    "        text = first_quote.find('span', class_='text').get_text()\n",
    "        author = first_quote.find('small', class_='author').get_text()\n",
    "        tags = [tag.get_text() for tag in first_quote.find_all('a', class_='tag')]\n",
    "        \n",
    "        print(\"\\nFirst quote:\" )\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Author: {author}\")\n",
    "        print(f\"Tags: {', '.join(tags)}\")\n",
    "else:\n",
    "    print(\"Failed to fetch the page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Scrape Multiple Pages\n",
    "\n",
    "Now let's scrape all available quotes from multiple pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all quotes\n",
    "quotes_data = scraper.scrape_quotes()\n",
    "\n",
    "print(f\"\\nTotal quotes scraped: {len(quotes_data)}\")\n",
    "\n",
    "# Display summary statistics\n",
    "if quotes_data:\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(quotes_data)\n",
    "    \n",
    "    print(f\"\\nUnique authors: {df['author'].nunique()}\")\n",
    "    print(f\"Most quoted author: {df['author'].mode().iloc[0]}\")\n",
    "    \n",
    "    # Show top 5 authors\n",
    "    print(\"\\nTop 5 authors by number of quotes:\")\n",
    "    print(df['author'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze the Scraped Data\n",
    "\n",
    "Let's perform some basic analysis on our scraped quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot top authors\n",
    "plt.figure(figsize=(12, 6))\n",
    "df['author'].value_counts().head(10).plot(kind='bar')\n",
    "plt.title('Top 10 Authors by Number of Quotes')\n",
    "plt.xlabel('Author')\n",
    "plt.ylabel('Number of Quotes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze quote lengths\n",
    "df['quote_length'] = df['text'].str.len()\n",
    "print(f\"\\nQuote length statistics:\")\n",
    "print(df['quote_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Data to CSV\n",
    "\n",
    "Finally, let's save our scraped data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "filepath = scraper.save_to_csv(quotes_data, 'demo_scraped_quotes.csv')\n",
    "\n",
    "# Verify the saved file\n",
    "if os.path.exists(filepath):\n",
    "    saved_df = pd.read_csv(filepath)\n",
    "    print(f\"Successfully saved {len(saved_df)} quotes to {filepath}\")\n",
    "    print(\"\\nFirst 5 rows of saved data:\")\n",
    "    print(saved_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we've successfully:\n",
    "1. Set up a web scraper with proper rate limiting\n",
    "2. Scraped quotes from multiple pages\n",
    "3. Extracted structured data (text, author, tags)\n",
    "4. Performed basic analysis on the scraped data\n",
    "5. Saved the results to a CSV file\n",
    "\n",
    "## Key Takeaways\n",
    "- Always be respectful when scraping (use delays, check robots.txt)\n",
    "- Handle errors gracefully\n",
    "- Structure your data for easy analysis\n",
    "- Save your work in standard formats like CSV\n",
    "\n",
    "## Next Steps\n",
    "- Try scraping different websites\n",
    "- Add more sophisticated error handling\n",
    "- Implement data validation\n",
    "- Explore APIs as alternatives to scraping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}